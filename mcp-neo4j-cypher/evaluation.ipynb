{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69f4e1a1-9324-4a2e-af91-c897ff1c5196",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a6c89ec-2dda-4f00-afb4-e9ed962f806f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import asyncio\n",
    "from typing import List, Tuple, Dict, Any\n",
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain.schema import AIMessage\n",
    "\n",
    "\n",
    "class MCPGraphEvaluator:\n",
    "    def __init__(self, evaluation_prompt: str, neo4j_config: Dict[str, str], namespace: str = \"graph\"):\n",
    "        \"\"\"\n",
    "        Initialize the MCP Graph Evaluator\n",
    "        \n",
    "        Args:\n",
    "            evaluation_prompt: The prompt template for evaluating answers\n",
    "            neo4j_config: Dictionary containing Neo4j connection parameters\n",
    "                         Should include: NEO4J_URI, NEO4J_USERNAME, NEO4J_PASSWORD, NEO4J_DATABASE\n",
    "            namespace: The namespace for the MCP server (default: \"graph\")\n",
    "        \"\"\"\n",
    "        self.client = None\n",
    "        self.agent = None\n",
    "        self.llm = None\n",
    "        self.evaluation_prompt = evaluation_prompt\n",
    "        self.neo4j_config = neo4j_config\n",
    "        self.namespace = namespace\n",
    "    \n",
    "    async def initialize(self):\n",
    "        \"\"\"Initialize the MCP client and agent\"\"\"\n",
    "        self.client = MultiServerMCPClient({\n",
    "            \"neo4j-graph\": {\n",
    "                \"command\": \"uvx\",\n",
    "                \"args\": [\"mcp-neo4j-cypher@0.2.4\", \"--namespace\", self.namespace],\n",
    "                \"transport\": \"stdio\",\n",
    "                \"env\": self.neo4j_config\n",
    "            }\n",
    "        })\n",
    "        \n",
    "        # Get tools from the client\n",
    "        tools = await self.client.get_tools()\n",
    "        \n",
    "        # Create the agent\n",
    "        self.agent = create_react_agent(\n",
    "            \"anthropic:claude-3-7-sonnet-latest\",\n",
    "            tools\n",
    "        )\n",
    "        \n",
    "        # Initialize the evaluation LLM\n",
    "        self.llm = ChatAnthropic(model='claude-3-5-haiku-latest')\n",
    "    \n",
    "    async def extract_tool_calls_and_final_answer(self, input_question: str) -> Tuple[List[Dict[str, Any]], str]:\n",
    "        \"\"\"Extract tool calls and final answer from agent response\"\"\"\n",
    "        tool_calls = []\n",
    "        final_answer = \"\"\n",
    "        \n",
    "        # Get response from agent\n",
    "        data = await self.agent.ainvoke({\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": input_question}]\n",
    "        })\n",
    "        \n",
    "        # Process messages to extract tool calls and final answer\n",
    "        for message in data[\"messages\"]:\n",
    "            if isinstance(message, AIMessage):\n",
    "                # Extract tool calls from the structured attribute\n",
    "                if hasattr(message, \"tool_calls\") and message.tool_calls:\n",
    "                    tool_calls.extend(message.tool_calls)\n",
    "                # Capture final plain text response\n",
    "                elif isinstance(message.content, str) and message.content.strip():\n",
    "                    final_answer = message.content\n",
    "        \n",
    "        return tool_calls, final_answer\n",
    "    \n",
    "    def extract_score_and_reasoning(self, text: str) -> Tuple[float, str]:\n",
    "        \"\"\"Extract score and reasoning from XML tags in text\"\"\"\n",
    "        score_pattern = r'<score>(.*?)</score>'\n",
    "        reasoning_pattern = r'<reasoning>(.*?)</reasoning>'\n",
    "        \n",
    "        score_match = re.search(score_pattern, text, re.IGNORECASE | re.DOTALL)\n",
    "        reasoning_match = re.search(reasoning_pattern, text, re.IGNORECASE | re.DOTALL)\n",
    "        \n",
    "        score = None\n",
    "        if score_match:\n",
    "            try:\n",
    "                score = float(score_match.group(1).strip())\n",
    "            except ValueError:\n",
    "                pass\n",
    "        \n",
    "        reasoning = reasoning_match.group(1).strip() if reasoning_match else None\n",
    "        \n",
    "        return (score, reasoning)\n",
    "    \n",
    "    async def evaluate_answer(self, record: Dict[str, str]) -> str:\n",
    "        \"\"\"Evaluate a generated answer against the reference answer\"\"\"\n",
    "        messages = [\n",
    "            (\"human\", self.evaluation_prompt.format(\n",
    "                question=record[\"question\"], \n",
    "                reference=record[\"answer\"], \n",
    "                generated_answer=record[\"generated_answer\"]\n",
    "            )),\n",
    "        ]\n",
    "        response = await self.llm.ainvoke(messages)\n",
    "        return response.content\n",
    "    \n",
    "    async def evaluate_record(self, record: Dict[str, str]) -> Dict[str, Any]:\n",
    "        \"\"\"Process a single record: generate answer and evaluate it\"\"\"\n",
    "        # Generate answer using the agent\n",
    "        tools, generated_answer = await self.extract_tool_calls_and_final_answer(record[\"question\"])\n",
    "        \n",
    "        # Update record with generated data\n",
    "        record['tools'] = tools\n",
    "        record['generated_answer'] = generated_answer\n",
    "        \n",
    "        # Evaluate the generated answer\n",
    "        evaluation_result = await self.evaluate_answer(record)\n",
    "        score, reasoning = self.extract_score_and_reasoning(evaluation_result)\n",
    "        \n",
    "        # Add evaluation results to record\n",
    "        record['evaluation_score'] = score\n",
    "        record['evaluation_reasoning'] = reasoning\n",
    "        record['evaluation_raw'] = evaluation_result\n",
    "        \n",
    "        return record\n",
    "    \n",
    "    async def evaluate_dataset(self, dataset: List[Dict[str, str]]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Evaluate an entire dataset\"\"\"\n",
    "        results = []\n",
    "        for record in dataset:\n",
    "            try:\n",
    "                result = await self.evaluate_record(record.copy())\n",
    "                results.append(result)\n",
    "                print(f\"Processed: {record['question'][:50]}...\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing record: {e}\")\n",
    "                record['error'] = str(e)\n",
    "                results.append(record)\n",
    "        \n",
    "        return results\n",
    "\n",
    "\n",
    "# Example usage\n",
    "async def evaluate_dataset(prompt: str, dataset: List[Dict[str, str]], neo4j_config: Dict[str, str], namespace: str = \"graph\"):\n",
    "    # Initialize the evaluator with the provided prompt and Neo4j config\n",
    "    evaluator = MCPGraphEvaluator(prompt, neo4j_config, namespace)\n",
    "    await evaluator.initialize()\n",
    "    \n",
    "    # Evaluate the dataset\n",
    "    results = await evaluator.evaluate_dataset(dataset)\n",
    "    \n",
    "    # Print results\n",
    "    for result in results:\n",
    "        print(f\"\\nQuestion: {result['question']}\")\n",
    "        print(f\"Reference Answer: {result['answer']}\")\n",
    "        print(f\"Generated Answer: {result.get('generated_answer', 'N/A')}\")\n",
    "        print(f\"Evaluation Score: {result.get('evaluation_score', 'N/A')}\")\n",
    "        print(f\"Evaluation Reasoning: {result.get('evaluation_reasoning', 'N/A')}\")\n",
    "        print(\"-\" * 80)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ede3774-734a-4674-99f2-b8210b2cadeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "neo4j_config = {\n",
    "    \"NEO4J_URI\": \"neo4j+s://demo.neo4jlabs.com\",\n",
    "    \"NEO4J_USERNAME\": \"recommendations\", \n",
    "    \"NEO4J_PASSWORD\": \"recommendations\",\n",
    "    \"NEO4J_DATABASE\": \"recommendations\"\n",
    "}\n",
    "\n",
    "# Example prompt\n",
    "prompt = \"\"\"You are an answer evaluation system. Compare the generated answer against the real answer and output only a single decimal score between 0 and 1.\n",
    "\n",
    "Scoring criteria:\n",
    "- 1.0: Generated answer is completely accurate and comprehensive\n",
    "- 0.8-0.9: Mostly accurate with minor omissions or slight inaccuracies\n",
    "- 0.6-0.7: Generally accurate but missing important details or contains some errors\n",
    "- 0.4-0.5: Partially accurate with significant gaps or notable errors\n",
    "- 0.2-0.3: Largely inaccurate with only some correct elements\n",
    "- 0.0-0.1: Completely inaccurate or irrelevant\n",
    "\n",
    "Consider both factual accuracy and completeness. Penalize hallucinations, contradictions, and missing key information.\n",
    "\n",
    "Input format:\n",
    "Question: {question}\n",
    "Real answer: {reference}\n",
    "Generated answer: {generated_answer}\n",
    "\n",
    "Output format:\n",
    "<reasoning>...</reasoning>\n",
    "<score>0.4</score>\n",
    "\"\"\"\n",
    "\n",
    "# Example dataset\n",
    "dataset = [\n",
    "    {\n",
    "        \"question\": \"How many movies in the graph?\", \n",
    "        \"answer\": \"There are 9,125 movies in the graph database.\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a28a7c5-c2fc-4cc6-a613-0678602138c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: How many movies in the graph?...\n",
      "\n",
      "Question: How many movies in the graph?\n",
      "Reference Answer: There are 9,125 movies in the graph database.\n",
      "Generated Answer: There are 9,125 movies in the graph database.\n",
      "Evaluation Score: 1.0\n",
      "Evaluation Reasoning: In this case, the generated answer is exactly the same as the real answer, matching perfectly in both content and phrasing. The factual information is 100% accurate and complete.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "results = await evaluate_dataset(prompt, dataset, neo4j_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5c13c1c-74c2-4953-b250-4d79dabbf69e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'question': 'How many movies in the graph?',\n",
       "  'answer': 'There are 9,125 movies in the graph database.',\n",
       "  'tools': [{'name': 'graph-read_neo4j_cypher',\n",
       "    'args': {'query': 'MATCH (m:Movie) RETURN COUNT(m) AS movieCount'},\n",
       "    'id': 'toolu_01LKXrKk9Ti6MsNJrHZf5vVY',\n",
       "    'type': 'tool_call'}],\n",
       "  'generated_answer': 'There are 9,125 movies in the graph database.',\n",
       "  'evaluation_score': 1.0,\n",
       "  'evaluation_reasoning': 'In this case, the generated answer is exactly the same as the real answer, matching perfectly in both content and phrasing. The factual information is 100% accurate and complete.',\n",
       "  'evaluation_raw': '<reasoning>In this case, the generated answer is exactly the same as the real answer, matching perfectly in both content and phrasing. The factual information is 100% accurate and complete.</reasoning>\\n<score>1.0</score>'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94367a0-bd9e-47f0-9ddf-0ac8fb7bc80e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
